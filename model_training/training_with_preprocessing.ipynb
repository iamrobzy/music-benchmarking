{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n",
      "Is GPU available?  True\n",
      "GPU Name:  NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import datasets\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Is GPU available? \", torch.cuda.is_available())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_HZ = 16000 # resampling rate in Hz\n",
    "MAX_LENGTH = 250000 # maximum audio interval length to consider (= RATE_HZ * SECONDS)\n",
    "AUDIO_FOLDER = Path(\"fma_small\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Experimental', 4: 'Rock', 5: 'International', 6: 'Electronic', 7: 'Instrumental'} \n",
      "\n",
      " {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Experimental': 3, 'Rock': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# this for large label mapping file\n",
    "# lableid_file = 'mappings.json'\n",
    "\n",
    "# # Read mappings back from the JSON file\n",
    "# with open(lableid_file, 'r') as f:\n",
    "#     mappings = json.load(f)\n",
    "#     label2id = mappings['label2id']\n",
    "#     id2label = mappings['id2label']\n",
    "\n",
    "# # Print loaded mappings to confirm\n",
    "# print(\"Loaded label2id:\", label2id)\n",
    "# print(\"Loaded id2label:\", id2label)\n",
    "# print(len(label2id))\n",
    "labels=['Hip-Hop','Pop','Folk','Experimental','Rock','International','Electronic','Instrumental']\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "print(id2label, '\\n\\n', label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   track_id    genre\n",
      "0         2  Hip-Hop\n",
      "1         5  Hip-Hop\n",
      "2        10      Pop\n",
      "3       140     Folk\n",
      "4       141     Folk\n",
      "   track_id  genre\n",
      "0         2      0\n",
      "1         5      0\n",
      "2        10      1\n",
      "3       140      2\n",
      "4       141      2\n",
      "   file  label\n",
      "0     2      0\n",
      "1     5      0\n",
      "2    10      1\n",
      "3   140      2\n",
      "4   141      2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torchaudio\n",
    "# this is for all tracks in FMA \n",
    "# with open('track_genres.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "# df = [{\"file\": track_id, \"label\": genre_id} for track_id, genre_id in data.items()]\n",
    "# dd = pd.DataFrame(df)\n",
    "# print(dd.head())\n",
    "# print(dd.shape)\n",
    "# print(dd['label'].value_counts())\n",
    "df1=pd.read_csv(\"track2genre_8000.csv\")\n",
    "print(df1.head())\n",
    "df1['genre']=df1['genre'].map(label2id)\n",
    "print(df1.head())\n",
    "df1.columns=['file','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_audio(file):\n",
    "    audio,rate = torchaudio.load(str(file))\n",
    "    transform = torchaudio.transforms.Resample(rate,RATE_HZ)\n",
    "    if audio.size(0) > 1:  # 如果是多通道\n",
    "        audio = audio.mean(dim=0)\n",
    "    audio = transform(audio).numpy()\n",
    "    audio = audio[:MAX_LENGTH]\n",
    "    return audio # truncate to first part of audio to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 tracks\n",
      "Processed 400 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1841] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 tracks\n",
      "Processed 800 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200 tracks\n",
      "Processed 1400 tracks\n",
      "Processed 1600 tracks\n",
      "Processed 1800 tracks\n",
      "Processed 2000 tracks\n",
      "Processed 2200 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (3328) too large for available bit count (3240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2400 tracks\n",
      "Processed 2600 tracks\n",
      "Processed 2800 tracks\n",
      "Processed 3000 tracks\n",
      "Processed 3200 tracks\n",
      "Processed 3400 tracks\n",
      "Processed 3600 tracks\n",
      "Processed 3800 tracks\n",
      "Processed 4000 tracks\n",
      "Processed 4200 tracks\n",
      "Processed 4400 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1365] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1365] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1365] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing fma_small/098/098565.mp3: Unspecified internal error.\n",
      "Error processing fma_small/098/098567.mp3: Unspecified internal error.\n",
      "Error processing fma_small/098/098569.mp3: Unspecified internal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1099] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing fma_small/099/099134.mp3: Error opening 'fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "Processed 4600 tracks\n",
      "Processed 4800 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1099] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing fma_small/108/108925.mp3: Error opening 'fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "Processed 5000 tracks\n",
      "Processed 5200 tracks\n",
      "Processed 5400 tracks\n",
      "Processed 5600 tracks\n",
      "Processed 5800 tracks\n",
      "Processed 6000 tracks\n",
      "Processed 6200 tracks\n",
      "Processed 6400 tracks\n",
      "Processed 6600 tracks\n",
      "Processed 6800 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1099] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing fma_small/133/133297.mp3: Error opening 'fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "Processed 7000 tracks\n",
      "Processed 7200 tracks\n",
      "Processed 7400 tracks\n",
      "Processed 7600 tracks\n",
      "Processed 7800 tracks\n",
      "Processed 8000 tracks\n",
      "Processed audio files: 7994\n",
      "Processed labels: 7994\n"
     ]
    }
   ],
   "source": [
    "audio_data = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in df1.iterrows():\n",
    "    track_id = str(row[\"file\"]).zfill(6)\n",
    "    label = row[\"label\"]\n",
    "    audio_path = AUDIO_FOLDER / track_id[:3] / f\"{track_id}.mp3\"\n",
    "    \n",
    "    if audio_path.exists():\n",
    "        try:\n",
    "            audio = get_transform_audio(audio_path)\n",
    "            audio_data.append(audio)\n",
    "            labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "    # else:\n",
    "    #     print(f\"not found: {audio_path}\")\n",
    "    if (idx + 1) % 200 == 0:\n",
    "        print(f\"Processed {idx + 1} tracks\")\n",
    "\n",
    "# 检查结果\n",
    "print(\"Processed audio files:\", len(audio_data))\n",
    "print(\"Processed labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "8\n",
      "Tensor dtype: torch.int64\n",
      "      label                                              audio\n",
      "6585      6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "6210      2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "7858      4  [0.04892895, 0.08216659, 0.09265838, 0.1291345...\n",
      "5450      7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3126      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "      label                                              audio\n",
      "6383      3  [-0.021570303, -0.040477242, -0.039989594, -0....\n",
      "5381      6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "6033      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1636      7  [0.007150736, 0.018054802, -0.17563047, -0.217...\n",
      "3042      6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(type(labels))\n",
    "CLASS_NUM=len(np.unique(labels))\n",
    "print(CLASS_NUM)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "print(\"Tensor dtype:\", labels.dtype)\n",
    "\n",
    "totdd = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"audio\": audio_data\n",
    "})\n",
    "# check the data\n",
    "totdd[\"audio\"] = totdd[\"audio\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "print(totdd.sample(5)) \n",
    "print(totdd['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      7994.00000\n",
      "mean     247341.76695\n",
      "std       25643.16009\n",
      "min           1.00000\n",
      "25%      250000.00000\n",
      "50%      250000.00000\n",
      "75%      250000.00000\n",
      "max      250000.00000\n",
      "Name: audio, dtype: float64\n",
      "0    250000\n",
      "1    250000\n",
      "2    250000\n",
      "3    250000\n",
      "4    250000\n",
      "Name: audio, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lengths = totdd[\"audio\"].apply(len)\n",
    "print(lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(totdd[\"audio\"].apply(len).nunique()) \n",
    "totdd = totdd[totdd[\"audio\"].apply(len) >= 16000]\n",
    "print(totdd[\"audio\"].apply(len).nunique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 992), (1, 997), (2, 985), (3, 971), (4, 994), (5, 992), (6, 978), (7, 1000)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, ClassLabel\n",
    "totdd = Dataset.from_pandas(totdd)\n",
    "\n",
    "from collections import Counter\n",
    "Counter(totdd['label']).items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f6734dc5534f35843533de695ebfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(totdd['label'][0]))\n",
    "totdd = totdd.map(lambda x: {\"label\": torch.tensor(x[\"label\"], dtype=torch.long)}, batched=True)\n",
    "print(type(totdd['label'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totdd = totdd.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'audio', '__index_level_0__'],\n",
      "        num_rows: 6327\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'audio', '__index_level_0__'],\n",
      "        num_rows: 1582\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'audio'],\n",
       "        num_rows: 6327\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'audio'],\n",
       "        num_rows: 1582\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(totdd)\n",
    "totdd['train'] = totdd['train'].remove_columns(['__index_level_0__'])\n",
    "totdd['test'] = totdd['test'].remove_columns(['__index_level_0__'])\n",
    "totdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbbcc6410454de892302c608812f71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87546302500d47ec91c81539b198057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8da8d2cc2254d4099fd6ea9afc048d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at dima806/music_genres_classification and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([10, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.570632\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "model_str = \"MIT/ast-finetuned-audioset-10-10-0.4593\" \n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_str)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_str,num_labels=len(label2id),ignore_mismatched_sizes=True)\n",
    "model.config.id2label = id2label\n",
    "# number of trainable parameters\n",
    "print(model.num_parameters(only_trainable=True)/1e6)\n",
    "print(len(label2id))\n",
    "print(len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff9bd56bec0431c811291cdc74999ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4eb36c355c463ba0c40d6bc7fc61f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(batch):    \n",
    "    inputs = feature_extractor(\n",
    "        batch['audio'], \n",
    "        sampling_rate=RATE_HZ, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True\n",
    "    )\n",
    "    inputs['input_values'] = inputs['input_values'][0]\n",
    "    \n",
    "    # Convert label to torch.long\n",
    "    if 'label' in batch:\n",
    "        inputs['labels'] = torch.tensor(batch['label'], dtype=torch.long)\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "totdd['test'] = totdd['test'].map(\n",
    "    preprocess_function, \n",
    "    remove_columns=totdd['test'].column_names,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "totdd['train'] = totdd['train'].map(\n",
    "    preprocess_function, \n",
    "    remove_columns=totdd['train'].column_names,\n",
    "    batched=False\n",
    ")\n",
    "totdd['train'].set_format(type='torch', columns=['input_values', 'labels'])\n",
    "totdd['test'].set_format(type='torch', columns=['input_values', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575bf5e5fc7543c1b05597e0ca80c79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/6327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9a3e67d8714c068291a5706d8def6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/1582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "totdd.save_to_disk(\"fma_small_class8_wave2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label type: <class 'torch.Tensor'>\n",
      "Training data label dtype: torch.int64\n",
      "Test data label type: <class 'torch.Tensor'>\n",
      "Test data label dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data label type:\", type(totdd['train'][0]['labels']))\n",
    "print(\"Training data label dtype:\", totdd['train'][0]['labels'].dtype)\n",
    "print(\"Test data label type:\", type(totdd['test'][0]['labels']))\n",
    "print(\"Test data label dtype:\", totdd['test'][0]['labels'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions  \n",
    "    label_ids = eval_pred.label_ids    \n",
    "    \n",
    "    present_classes = np.unique(label_ids)\n",
    "    # print(f\"Present classes in current dataset: {present_classes}\")\n",
    "    # print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    \n",
    "    predictions = np.exp(predictions)/np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    \n",
    "    acc_score = accuracy.compute(\n",
    "        predictions=predictions.argmax(axis=1),\n",
    "        references=label_ids\n",
    "    )['accuracy']\n",
    "    \n",
    "    y_true_bin = label_binarize(label_ids, classes=present_classes)\n",
    "    \n",
    "    predictions_subset = predictions[:, present_classes]\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(\n",
    "            y_true=y_true_bin,\n",
    "            y_score=predictions_subset,\n",
    "            multi_class='ovr',\n",
    "            average='macro'\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC AUC calculation error: {e}\")\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"accuracy\": acc_score,\n",
    "        \"present_classes\": len(present_classes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Test labels: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels:\", totdd[\"train\"][\"labels\"].unique())\n",
    "print(\"Test labels:\", totdd[\"test\"][\"labels\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size=5\n",
    "warmup_steps=50\n",
    "weight_decay=0.02\n",
    "num_train_epochs=10\n",
    "model_name = \"fma_small_finetuned_1.0_AST\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=5e-5, # 5e-6\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,#1\n",
    "    evaluation_strategy='epoch',\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_steps=20,#1\n",
    "    gradient_accumulation_steps=1, \n",
    "    gradient_checkpointing=True,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1, # save fewer checkpoints to limit used space\n",
    "    report_to=\"mlflow\",  # log to mlflow\n",
    "\n",
    "    optim=\"adamw_hf\",  # Optimizer, in this case Adam\n",
    "    adam_beta1=0.9,  # Adam optimizer beta1\n",
    "    adam_beta2=0.999,  # Adam optimizer beta2\n",
    "    adam_epsilon=1e-8,  # Adam optimizer epsilon\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=totdd[\"train\"],\n",
    "    eval_dataset=totdd[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='634' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/317 04:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1797096729278564,\n",
       " 'eval_model_preparation_time': 0.0032,\n",
       " 'eval_roc_auc': 0.43770623958633986,\n",
       " 'eval_accuracy': 0.07901390644753477,\n",
       " 'eval_present_classes': 8,\n",
       " 'eval_runtime': 24.3378,\n",
       " 'eval_samples_per_second': 65.002,\n",
       " 'eval_steps_per_second': 13.025}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12660' max='12660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12660/12660 42:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Present Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.011400</td>\n",
       "      <td>1.928674</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.655313</td>\n",
       "      <td>0.213654</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.082900</td>\n",
       "      <td>2.083765</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.494915</td>\n",
       "      <td>0.125158</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.066600</td>\n",
       "      <td>2.075129</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.533946</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.105300</td>\n",
       "      <td>2.082439</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.507632</td>\n",
       "      <td>0.118837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.083400</td>\n",
       "      <td>2.079763</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.512423</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.091400</td>\n",
       "      <td>2.080866</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.501181</td>\n",
       "      <td>0.118837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.080600</td>\n",
       "      <td>2.079350</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.494039</td>\n",
       "      <td>0.123262</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.083800</td>\n",
       "      <td>2.080013</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.512517</td>\n",
       "      <td>0.118837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.081100</td>\n",
       "      <td>2.080103</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.503930</td>\n",
       "      <td>0.118837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.083300</td>\n",
       "      <td>2.080346</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.499573</td>\n",
       "      <td>0.118837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12660, training_loss=2.064557850247876, metrics={'train_runtime': 2527.9804, 'train_samples_per_second': 25.028, 'train_steps_per_second': 5.008, 'total_flos': 8.97522582996031e+18, 'train_loss': 2.064557850247876, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/317 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9286738634109497,\n",
       " 'eval_model_preparation_time': 0.0032,\n",
       " 'eval_roc_auc': 0.6553131521179113,\n",
       " 'eval_accuracy': 0.213653603034134,\n",
       " 'eval_present_classes': 8,\n",
       " 'eval_runtime': 22.146,\n",
       " 'eval_samples_per_second': 71.435,\n",
       " 'eval_steps_per_second': 14.314,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('for_infere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.26550349593162537, 'label': 'Folk'},\n",
       " {'score': 0.15864621102809906, 'label': 'Rock'},\n",
       " {'score': 0.13659189641475677, 'label': 'International'},\n",
       " {'score': 0.12934762239456177, 'label': 'Experimental'},\n",
       " {'score': 0.1281781941652298, 'label': 'Instrumental'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe=pipeline('audio-classification',model='for_infere',device=0,return_all_scores=True)\n",
    "audio,rate=torchaudio.load('fma_small/032/032330.mp3')\n",
    "transform=torchaudio.transforms.Resample(rate,RATE_HZ)\n",
    "audio1=transform(audio).numpy().reshape(-1)\n",
    "# make a classification pipeline\n",
    "pipe(audio1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logitsmodel = AutoModelForAudioClassification.from_pretrained('for_infere')\n",
    "# from transformers import AutoProcessor\n",
    "# processor = AutoProcessor.from_pretrained('for_infere')\n",
    "# audio2=transform(audio).squeeze(0) \n",
    "# inputs = processor(audio2, sampling_rate=RATE_HZ, return_tensors=\"pt\",padding=True)\n",
    "# with torch.no_grad():\n",
    "#     logits = logitsmodel(**inputs).logits\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model_save_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRAINenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
